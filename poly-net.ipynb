{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.attention import Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from src.transformers import ViT\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, embed_dim=8):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.encoder = ViT(embed_dim=embed_dim, n_blocks=2, n_heads=4)\n",
    "        self.decoder = ViT(embed_dim=embed_dim,n_blocks=2, n_heads=4,\n",
    "                           img_shape=(1, 2, embed_dim), patch_size=2)\n",
    "        \n",
    "        self.mapper = nn.Linear(3, embed_dim)\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 3),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x, p):\n",
    "        b = x.shape[0]\n",
    "        x = self.encoder(x)\n",
    "        \n",
    "        p = self.mapper(p).view(b, 1, 1, -1)\n",
    "        x = x[:, 0, :].view(b, 1, 1, -1)\n",
    "        x = torch.cat((p,x), dim=2)\n",
    "\n",
    "        x = self.decoder(x)\n",
    "        x = self.mlp(x[:, 0, :])\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 2, 2])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T = 2\n",
    "torch.tril(torch.ones(T, T)).view(1, 1, T, T).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define dataset anda dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.datasets import PolyMNIST\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "train = PolyMNIST(csv_file=\"mnist/train/polygon-mnist.csv\",\n",
    "                  transform=ToTensor())\n",
    "\n",
    "test = PolyMNIST(csv_file=\"mnist/test/polygon-mnist.csv\",\n",
    "                  transform=ToTensor())\n",
    "\n",
    "train_loader = DataLoader(train, batch_size=60, shuffle=True)\n",
    "test_loader = DataLoader(test, batch_size=60, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:  cuda (Quadro M5000)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11541d25506e4e188ea9a02929a4233c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a807519614641efad08d9f8691899dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1 in training:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 35\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# That's lame\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# I urge to learn how to do masking in the attention and use it\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# At least the loss is decreasing\u001b[39;00m\n\u001b[1;32m     34\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m11\u001b[39m):\n\u001b[1;32m     36\u001b[0m     pred \u001b[38;5;241m=\u001b[39m model(image, polygon[:, i, :])\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n",
      "Cell \u001b[0;32mIn[5], line 35\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# That's lame\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# I urge to learn how to do masking in the attention and use it\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# At least the loss is decreasing\u001b[39;00m\n\u001b[1;32m     34\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m11\u001b[39m):\n\u001b[1;32m     36\u001b[0m     pred \u001b[38;5;241m=\u001b[39m model(image, polygon[:, i, :])\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1457\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:701\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1152\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1135\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:312\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/paulo/img2seq/.venv/lib/python3.10/site-packages/debugpy/_vendored/pydevd/pydevd.py:2070\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[1;32m   2067\u001b[0m             from_this_thread\u001b[38;5;241m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[1;32m   2069\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads_suspended_single_notification\u001b[38;5;241m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[0;32m-> 2070\u001b[0m         keep_suspended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuspend_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_this_thread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes_tracker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2072\u001b[0m frames_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2074\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n\u001b[1;32m   2075\u001b[0m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[0;32m~/paulo/img2seq/.venv/lib/python3.10/site-packages/debugpy/_vendored/pydevd/pydevd.py:2106\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[1;32m   2103\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_input_hook()\n\u001b[1;32m   2105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_internal_commands()\n\u001b[0;32m-> 2106\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mid\u001b[39m(frame)))\n\u001b[1;32m   2110\u001b[0m \u001b[38;5;66;03m# process any stepping instructions\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.optim import Adam\n",
    "from torch.nn import MSELoss\n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "N_EPOCHS = 30\n",
    "LR = 0.0007\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device_name = torch.cuda.get_device_name(\n",
    "    device) if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device: \", device, f\"({device_name})\")\n",
    "\n",
    "model = Net().to(device)\n",
    "optimizer = Adam(model.parameters(), lr=LR)\n",
    "criterion = MSELoss()\n",
    "\n",
    "model.train()\n",
    "train_loss = 0.0\n",
    "for epoch in trange(N_EPOCHS, desc=\"Training\"):\n",
    "\n",
    "    desc = f\"Epoch {epoch + 1} in training\"\n",
    "\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for batch in tqdm(train_loader, desc=desc, leave=False):\n",
    "        image, _, polygon = batch\n",
    "\n",
    "        image = image.to(device)\n",
    "        polygon = polygon.to(device)\n",
    "        \n",
    "        # That's lame\n",
    "        # I urge to learn how to do masking in the attention and use it\n",
    "        # At least the loss is decreasing\n",
    "        loss = None\n",
    "        for i in range(11):\n",
    "            pred = model(image, polygon[:, i, :])\n",
    "\n",
    "            if (loss is None):\n",
    "                loss = criterion(pred, polygon[:, (i + 1), :])\n",
    "            else:\n",
    "                loss += criterion(pred, polygon[:, (i + 1), :])\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        train_loss += loss.item() / len(train_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{N_EPOCHS} loss: {train_loss:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3931d7b5c60e4e36a6fea987520c2523",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing:   0%|          | 0/167 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.04\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    correct, total = 0, 0\n",
    "    test_loss = 0.0\n",
    "    for batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "        x, _, p = batch\n",
    "        x, p = x.to(device), p.to(device)\n",
    "        \n",
    "        loss = None\n",
    "        for i in range(11):\n",
    "            pred = model(x, p[:, i, :])\n",
    "\n",
    "            if loss is None:\n",
    "                 loss = criterion(pred, p[:, (i + 1), :])\n",
    "            else:\n",
    "                loss += criterion(pred, p[:, (i + 1), :])\n",
    "\n",
    "        test_loss += loss.detach().cpu().item() / len(test_loader)\n",
    "\n",
    "        # correct += torch.sum(torch.argmax(y_hat, dim=1)\n",
    "        #                      == y).detach().cpu().item()\n",
    "        total += len(x)\n",
    "    print(f\"Test loss: {test_loss:.2f}\")\n",
    "    # print(f\"Test accuracy: {correct / total * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[1;32m      4\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmatplotlib\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minline\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m batch_images, _, poly \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(\u001b[43mtest_loader\u001b[49m))\n\u001b[1;32m      7\u001b[0m idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m      9\u001b[0m img \u001b[38;5;241m=\u001b[39m batch_images[idx]\u001b[38;5;241m.\u001b[39mnumpy()[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m255\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_loader' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "%matplotlib inline\n",
    "\n",
    "batch_images, _, poly = next(iter(test_loader))\n",
    "idx = 0\n",
    "\n",
    "img = batch_images[idx].numpy()[0] * 255\n",
    "img = img.astype(np.uint8)\n",
    "\n",
    "\n",
    "np_out = np.zeros((28,28, 3))\n",
    "np_out[:,:, 1] = np.copy(img)\n",
    "\n",
    "pts = poly[idx].numpy()#.reshape((-1, 3))\n",
    "pts = np.delete(pts, 0, 1).reshape(-1, 1, 2) * 28\n",
    "pts = pts[1:7].astype(np.int32)\n",
    "image = cv2.polylines(np_out, [pts], True, (255,0,0), 1)\n",
    "\n",
    "plt.title(\"original\")\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1377, 0.2718, 0.4272, 0.5817, 0.7351, 0.8810], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    inputs = torch.zeros(1, 3).to(device)\n",
    "    image = batch_images[idx].reshape(1, 1, 28, 28).to(device)\n",
    "\n",
    "    poly = []\n",
    "    for i in range(10):\n",
    "        pred = model(image, inputs)\n",
    "\n",
    "        if (pred[0][0] > 0.95):\n",
    "            break\n",
    "        poly.append(pred)\n",
    "        inputs = pred\n",
    "    poly = torch.cat(poly)\n",
    "\n",
    "    np_out = np.zeros((28,28, 3))\n",
    "    np_out[:,:, 1] = np.copy(img)\n",
    "    \n",
    "    print(poly[:, 0])\n",
    "    pts = poly.to('cpu').numpy() * 28\n",
    "    pts = np.delete(pts, 0, 1).reshape(-1, 1, 2)\n",
    "    pts = pts.astype(np.int32)\n",
    "    image = cv2.polylines(np_out, [pts], True, (255,0,0), 1)\n",
    "    \n",
    "    plt.title(\"prediction\")\n",
    "    plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"checkpoints/polygon_004.pth\"\n",
    "torch.save(model.state_dict(), path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
